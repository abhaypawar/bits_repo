Of course. Here is a comprehensive markdown document detailing the entire project, from the core concepts and file architecture to the novel ideas and step-by-step workflow, as requested.

---

# Deep Dive: The MCP-SRE Agent Platform

## 1. Project Overview: What is this Tool?

The **Multi-agent Collaborative Platform for Site Reliability Engineering (MCP-SRE)** is a proof-of-concept system designed to automate and augment the incident analysis workflow for software engineers. It tackles the complex and often time-consuming process of diagnosing production failures, proposing fixes, and documenting the event.

At its core, the project simulates a real-world scenario:
1.  **An application fails in production**, generating a stream of complex and often noisy logs.
2.  A **crew of specialized AI agents** collaborates to analyze these logs, understand the root cause by examining the source code, formulate a remediation plan, and write a comprehensive postmortem report.

The project's central innovation is its departure from traditional Retrieval-Augmented Generation (RAG). Instead of relying on semantic similarity search over a vector database, it employs a **Code Intelligence Graph**, a structured knowledge base that represents the application's source code as a network of interconnected components. This enables a more precise, causal form of reasoning for the AI agents.



---
## 2. System Architecture: The Idea Behind Each File

Each file in this project plays a distinct and crucial role in the overall ecosystem. They are divided into three main categories: **Simulation**, **Knowledge Base**, and **Analysis Platform**.

### Simulation (The "Problem Space")
These files create the realistic incidents and the flawed application.

* `enhanced_ecommerce_runner.py`
    * **Idea:** To act as a "Chaos Monkey" or an incident simulator. It doesn't just generate random errors; it simulates specific, categorized failure scenarios (e.g., database deadlocks, API rate limiting, memory leaks) that an SRE would encounter in a real microservices-based e-commerce platform.
    * **What it does:** Runs a simulation loop that randomly triggers one of its pre-defined incident scenarios. It then generates realistic application logs, access logs, and detailed error logs, saving them to the `logs/` directory. **This is the source of the input for our analysis.**

* `buggy_app.py`
    * **Idea:** To serve as the "ground truth" source code for the simulated application. It's a deliberately flawed Flask application where the bugs correspond directly to the incidents generated by the runner.
    * **What it does:** Contains the actual Python code with vulnerabilities, logical errors, and misconfigurations (e.g., functions that acquire locks in the wrong order, missing environment variable checks, inefficient database queries). The AI agents analyze this file to find the root cause.

### Knowledge Base (The "Brain")
These files are used to build the system's understanding of the `buggy_app.py` code before any analysis begins.

* `build_graph.py`
    * **Idea:** To act as a "cartographer" for the source code. It reads the `buggy_app.py` file and maps out all its components and their relationships.
    * **What it does:** Uses Python's Abstract Syntax Tree (`ast`) module to parse the source code. It identifies functions, classes, and variables, and then builds a `networkx` graph. It also enriches this graph with manual mappings (e.g., connecting a function to an error type it can cause). This script is run only once during setup.

* `code_intelligence_graph.graphml`
    * **Idea:** To be the persistent, queryable "brain" of the RCA agent.
    * **What it does:** This is the output file from `build_graph.py`. It's a structured representation of the codebase, storing nodes (like `Function`, `ErrorType`) and edges (like `CALLS`, `MODIFIES`) that the `CodeGraphTool` can query during an analysis.

### Analysis Platform (The "Solution Space")
These files constitute the interactive application that receives incident data and performs the analysis.

* `mcp_server.py`
    * **Idea:** To be the central orchestrator of the AI agent crew.
    * **What it does:** Runs a Flask web server that listens for requests from the Gradio UI. When it receives an incident log, it instantiates the agents and tasks, builds the appropriate crew based on the user's desired analysis level, and streams the results back to the user.

* `mcp_host_gradio.py`
    * **Idea:** To be the user-friendly "front door" to the entire MCP system.
    * **What it does:** Launches a web interface using Gradio. It provides a UI for the user to either paste a log snippet or upload a log file, select the depth of analysis, and view the results in real-time. It also handles the download functionality for the final report.

* `code_graph_tool.py`
    * **Idea:** To be the specialized "searchlight" for the Root Cause Analysis agent. It's the agent's interface to the knowledge graph.
    * **What it does:** Defines a custom CrewAI tool (`@tool`) that allows the RCA agent to perform structured queries against the `code_intelligence_graph.graphml` file. For example, the agent can ask, "Which functions can cause a 'database_deadlock'?"

* `llm_provider.py`
    * **Idea:** To decouple the application logic from the specific Large Language Model being used.
    * **What it does:** Contains a factory function (`get_llm`) that reads the `.env` file and initializes the correct LangChain LLM client (e.g., for OpenRouter, OpenAI, etc.). This makes the system model-agnostic.

---
## 3. The Workflow Sequence

The project is used in a clear, two-part sequence: first, you generate a problem, then you solve it.

1.  **Setup:** The user first sets up the Python environment and runs `python build_graph.py` once to create the knowledge graph.
2.  **Generate an Incident:** The user runs `python enhanced_ecommerce_runner.py`. This script populates the `logs/` directory with new incident data.
3.  **Find an Input Log:** The user opens a file like `logs/error_...log`, finds an interesting error message, and copies it (or notes the file path).
4.  **Start the MCP:** The user opens two terminals.
    * In Terminal 1, they start the analysis backend: `python mcp_server.py`.
    * In Terminal 2, they start the user interface: `python mcp_host_gradio.py`.
5.  **Analyze the Incident:** In the Gradio web UI, the user provides the log data, selects an analysis level, and clicks "Analyze."
6.  **Receive Results:** The server runs the AI agent crew, which uses the `CodeGraphTool` to query the graph. The results are streamed back to the Gradio UI in real-time.

---
## 4. The Novel Idea: Code Intelligence Graph vs. Traditional RAG

This project's unique contribution is its **Code Intelligence Graph RAG** system.

### 4.1. Why Traditional RAG Was Avoided
Traditional RAG works by converting documents into numerical vectors and storing them in a vector database. When you ask a question, it finds the "most semantically similar" chunks of text. This is powerful for prose but has significant demerits for code analysis:

* **Correlation over Causation:** It can find code that *mentions* a "deadlock" but may fail to find the two specific, unrelated-looking functions that *cause* the deadlock through their interaction.
* **Lack of Structural Understanding:** It doesn't understand explicit relationships like "function A calls function B" or "function C modifies database table X." It only knows that the text of these functions might be similar.
* **High Noise, Low Signal:** A query for a simple error might return dozens of irrelevant code chunks, increasing the cognitive load on the LLM and leading to hallucinations.

### 4.2. The Graph Model Advantage
Our approach converts the codebase into a structured graph, which allows for a fundamentally different type of retrieval.

* **Nodes (The Entities):** `Service`, `File`, `Function`, `DatabaseTable`, `ErrorType`.
* **Edges (The Relationships):** `CONTAINS`, `IMPLEMENTS`, `CALLS`, `MODIFIES`, `CAN_CAUSE`.

When an incident occurs (e.g., "DATABASE DEADLOCK DETECTED"), the RCA agent doesn't perform a similarity search. It performs a precise graph traversal query via the `CodeGraphTool`:

> **Query 1:** "Find all `Function` nodes that have a `CAN_CAUSE` relationship with the `database_deadlock` `ErrorType` node."
> **Result:** `create_order`, `process_inventory_update`.
>
> **Query 2:** "For these two functions, show me which `DatabaseTable` nodes they have a `MODIFIES` relationship with."
> **Result:** Both functions modify `INVENTORY` and `ORDERS`.

This retrieves only the two exact functions involved, providing the LLM with a perfect, noise-free context to analyze the lock ordering and find the root cause.

### 4.3. Merits and Demerits of the Graph Approach

#### Merits (Advantages)
* **Precision:** Retrieves only the most relevant code, dramatically reducing noise.
* **Causal Reasoning:** The retrieval is based on explicit, causal relationships, not just semantic similarity.
* **Explainability:** The path taken through the graph to find the answer is, in itself, an explanation of the root cause.
* **Reduced Hallucination:** By providing a perfect, minimal context, the LLM is far less likely to hallucinate or make incorrect assumptions.

#### Demerits (Disadvantages)
* **Upfront Cost:** Requires building a parser (`build_graph.py`) to create the graph, which is more complex than simply embedding text files.
* **Maintenance Overhead:** If the `buggy_app.py` source code changes significantly, the graph must be rebuilt to remain accurate.

---
## 5. Gradio UI: Flexible Input Methods

The Gradio interface is designed for flexibility, mirroring how an SRE might approach an incident. The MCP can receive input in two primary ways:

1.  **Pasted Text Snippet:** For quick analysis, an SRE can copy a single, critical error line from their monitoring tool and paste it directly into the textbox. The AI is prompted to find the key error type from this snippet.
2.  **Uploaded Log File:** For more complex incidents, the SRE can upload a raw `.log` or `.txt` file containing many log entries. The AI is prompted to first scan this larger file to identify the most critical patterns before beginning its deeper analysis.

This dual-input system makes the tool adaptable to both simple and complex incident investigation scenarios.
